{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each phase of the process:**\n",
    "1. [Business understanding](#Businessunderstanding)\n",
    "    1. [What Questions Are We Trying to Answer?](#QA)\n",
    "        1. [What are the Desired Outputs](#Desiredoutputs)\n",
    "2. [Data Understanding](#Dataunderstanding)\n",
    "    1. [Initial Data Report](#Datareport)\n",
    "    2. [Describe Data](#Describedata)\n",
    "    3. [Initial Data Exploration](#Exploredata) \n",
    "    4. [Verify Data Quality](#Verifydataquality)\n",
    "        1. [Missing Data](#MissingData) \n",
    "        2. [Outliers](#Outliers)\n",
    "3. [Data Preparation](#Datapreparation)\n",
    "    1. [Select Your Data](#Selectyourdata)\n",
    "    2. [Cleanse the Data](#Cleansethedata)\n",
    "        1. [Label Encoding](#labelEncoding)\n",
    "        2. [Drop Unnecessary Columns](#DropCols)\n",
    "        3. [Altering Datatypes](#AlteringDatatypes)\n",
    "        4. [Dealing With Zeros](#DealingZeros)\n",
    "    3. [Construct Required Data](#Constructrequireddata)\n",
    "    4. [Integrate Data](#Integratedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What Questions Are We Trying To Answer? <a class=\"anchor\" id=\"QA\"></a>\n",
    "\n",
    "\n",
    "In this project - we are using datasets from Kaggle - HEART DISEASE ANALYSIS and applying several ML Models, such as KNN, Linear Regression, Decision Tree, for  predicting heart illness of patients by the characteristics provided by the data set. However, we aren't looking for the best algorithm, but to see how each of them performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point out all library that we are using here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Seaborn\n",
    " - Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, mean_squared_error,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "\n",
    "# Setting Configs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stage  Two - Data Understanding <a class=\"anchor\" id=\"Dataunderstanding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description \n",
    "\n",
    "This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n",
    "\n",
    "**The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.**\n",
    "\n",
    "https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "\n",
    "## Attribute Information\n",
    "\n",
    "- Age (age in years)\n",
    "- Sex (1 = male; 0 = female)\n",
    "- CP (chest pain type)\n",
    "- TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))\n",
    "- CHOL (serum cholestoral in mg/dl)\n",
    "- FPS (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "- RESTECH (resting electrocardiographic results)\n",
    "- THALACH (maximum heart rate achieved)\n",
    "- EXANG (exercise induced angina (1 = yes; 0 = no))\n",
    "- OLDPEAK (ST depression induced by exercise relative to rest)\n",
    "- SLOPE (the slope of the peak exercise ST segment)\n",
    "- CA (number of major vessels (0-3) colored by flourosopy)\n",
    "- THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)\n",
    "- TARGET (1 or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Describe Data <a class=\"anchor\" id=\"Describedata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "heart = pd.read_csv('./datasets/heart.csv')\n",
    "\n",
    "# Inspecting Dataset\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print('#' * 50)\n",
    "print('Total Rows:', heart.shape[0])\n",
    "print('Total Columns:', heart.shape[1])\n",
    "print('#' * 50, '\\n')\n",
    "heart.info(memory_usage=False)\n",
    "print('\\n')\n",
    "print('#' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "print('#' * 64)\n",
    "print('Descriptive Statistics')\n",
    "print('#' * 64)\n",
    "heart.select_dtypes(exclude='object').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Data Quality <a class=\"anchor\" id=\"Verifydataquality\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Missing Data <a class=\"anchor\" id=\"MissingData\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print('#' * 35)\n",
    "print('Checking NA Values ')\n",
    "print('#' * 35)\n",
    "print(heart.isna().sum())\n",
    "print('#' * 35,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print('#' * 35)\n",
    "print('Checking NULL Values ')\n",
    "print('#' * 35)\n",
    "print(heart.isnull().sum())\n",
    "print('#' * 35,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Outliers <a class=\"anchor\" id=\"Outliers\"></a>\n",
    "\n",
    "At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the definition of extreme outliers:\n",
    "\n",
    "https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
    "\n",
    "- Below the first quartile − 3 ∗ interquartile range\n",
    "- Above the third quartile + 3 ∗ interquartile range\n",
    "\n",
    "\n",
    "\n",
    "**We didn't found any outliers in our dataset, or odd figures that could affect our analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Initial Data Exploration  <a class=\"anchor\" id=\"Exploredata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.suptitle('Sex Distribution by Age', \n",
    "             fontweight='heavy', \n",
    "             fontsize='16', fontfamily='sans-serif'\n",
    "            )\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=heart[heart.sex == 0], x=heart.age[heart.sex == 0], color='#3597e8',label ='Female')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=heart[heart.sex == 1], x=heart.age[heart.sex == 1], color='#ff964f', label='Male')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sex', data=heart)\n",
    "plt.xlabel(\"Sex (0 = Man, 1= Woman)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(heart.cp, heart.target).plot(kind='bar')\n",
    "plt.title(\"Frequência de doença cardíaca em relação ao tipo de dor\")\n",
    "plt.xlabel(\"Tipo de dor \")\n",
    "plt.ylabel(\"tem a doença ou nao\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart['age'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(heart.corr(), linewidth = 0.1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=heart, x='target', y='age', hue='sex');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Clean The Data <a class=\"anchor\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing and inspecting datatypes\n",
    "print('#' * 35)\n",
    "print('Fixing Data Type')\n",
    "print('#' * 35)\n",
    "fix_dtype          = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "heart[fix_dtype]   = heart[fix_dtype].astype(object)\n",
    "print(heart[fix_dtype].dtypes)\n",
    "print('#' * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Enconding -> Label Encoding to turn Categorical values to Integers\n",
    "# Dropping unecessary columns\n",
    "df = pd.get_dummies(heart, columns=['cp', 'thal', 'slope'], drop_first=True)\n",
    "\n",
    "# Inspecting new dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset in 2: Dependent (Target) variable and Independent Variables\n",
    "X = df.loc[:, df.columns != 'target'].values\n",
    "\n",
    "y = df.target.values\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1984)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting X and y\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the steps\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('logreg', LogisticRegression())]\n",
    "         \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create the parameter space\n",
    "parameters = {\"logreg__C\": np.linspace(0.001, 1.0, 20)}\n",
    "\n",
    "# Instantiate the grid search object\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit to the training data\n",
    "cv.fit(X_train, y_train)\n",
    "print(cv.best_score_, \"\\n\", cv.best_params_)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R-squared\n",
    "r_squared = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"R^2: {}\".format(r_squared))\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KFold object\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "\n",
    "# Compute 6-fold cross-validation scores\n",
    "cv_scores = cross_val_score(reg ,X, y, cv=kf)\n",
    "\n",
    "# Print scores\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean\n",
    "print(np.mean(cv_scores))\n",
    "\n",
    "# Print the standard deviation\n",
    "print(np.std(cv_scores))\n",
    "\n",
    "# Print the 95% confidence interval\n",
    "print(np.quantile(cv_scores, [0.025, 0.975]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 40)\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "error_rate = []\n",
    "\n",
    "for neighbor in neighbors:\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "\n",
    "\t# Compute accuracy\n",
    "    train_accuracies[neighbor] = knn.score(X_train, y_train)\n",
    "    test_accuracies[neighbor] = knn.score(X_test, y_test)\n",
    "\n",
    "print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a title\n",
    "plt.title(\"KNN: Varying Number of Neighbors\")\n",
    "\n",
    "# Plot training accuracies\n",
    "plt.plot(neighbors,train_accuracies.values(), label=\"Training Accuracy\")\n",
    "\n",
    "# Plot test accuracies\n",
    "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models dictionary\n",
    "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\n",
    "results = []\n",
    "\n",
    "# Loop through the models' values\n",
    "for model in models.values():\n",
    "  \n",
    "  # Instantiate a KFold object\n",
    "  kf = KFold(n_splits=6, random_state=1984, shuffle=True)\n",
    "  \n",
    "  # Perform cross-validation\n",
    "  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
    "  results.append(cv_results)\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set perfomance\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    print(\"{} Test Set Accuracy: {:.2f}% \".format(name, test_score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "  \n",
    "  # Fit the model to the training data\n",
    "  model.fit(X_train_scaled,y_train)\n",
    "  \n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test_scaled)\n",
    "  \n",
    "  # Calculate the test_rmse\n",
    "  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "  print(\"Model {} \\n\".format(name))\n",
    "  print(\"Test Set RMSE: {} \\n\".format(test_rmse))\n",
    "  print(\"Classification Report: \\n {}\".format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "# DOT data\n",
    "dot_data = tree.export_graphviz(cv, out_file=None, \n",
    "                                feature_names=df.loc[:, df.columns != 'target'].columns,  \n",
    "                                class_names=heart.target.unique(),\n",
    "                                filled=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns != 'target'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cdc2136a0c3ff6ee581d9355ebf305035e9e0ffdb59b37da8e4674310bf0233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
